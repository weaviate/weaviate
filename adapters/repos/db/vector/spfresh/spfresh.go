//                           _       _
// __      _____  __ ___   ___  __ _| |_ ___
// \ \ /\ / / _ \/ _` \ \ / / |/ _` | __/ _ \
//  \ V  V /  __/ (_| |\ V /| | (_| | ||  __/
//   \_/\_/ \___|\__,_| \_/ |_|\__,_|\__\___|
//
//  Copyright Â© 2016 - 2025 Weaviate B.V. All rights reserved.
//
//  CONTACT: hello@weaviate.io
//

package spfresh

import (
	"context"
	"encoding/binary"
	"fmt"
	"math"
	"sync"
	"sync/atomic"

	"github.com/pkg/errors"
	"github.com/puzpuzpuz/xsync/v4"
	"github.com/sirupsen/logrus"
	"github.com/weaviate/weaviate/adapters/repos/db/helpers"
	"github.com/weaviate/weaviate/adapters/repos/db/vector/common"
	"github.com/weaviate/weaviate/adapters/repos/db/vector/compressionhelpers"
	"github.com/weaviate/weaviate/adapters/repos/db/vector/hnsw/distancer"
	"github.com/weaviate/weaviate/adapters/repos/db/vector/hnsw/visited"
	enterrors "github.com/weaviate/weaviate/entities/errors"
	schemaConfig "github.com/weaviate/weaviate/entities/schema/config"
)

const (
	reassignThreshold = 3        // Fine-tuned threshold to avoid unnecessary splits during reassign operations
	splitReuseEpsilon = 0.000001 // Epsilon to determine if a split can reuse the existing posting
)

var (
	ErrPostingNotFound  = errors.New("posting not found")
	ErrVectorNotFound   = errors.New("vector not found")
	ErrIdenticalVectors = errors.New("posting list contains identical or near-identical vectors")
)

// SPFresh manages background operations for postings in the SPFresh index.
// It handles split, merge, and reassign requests generated by insertions or during search
// operations. It uses a worker pool to process these operations concurrently.
type SPFresh struct {
	Logger       *logrus.Entry
	UserConfig   *UserConfig                      // UserConfig contains user-defined settings for the rebuilder.
	SPTAG        SPTAG                            // SPTAG provides access to the SPTAG index for centroid operations.
	Store        *LSMStore                        // Used for managing persistence of postings.
	VersionMap   *VersionMap                      // Provides access to vector versions.
	IDs          *common.MonotonicCounter[uint64] // Shared monotonic counter for generating unique IDs for new postings.
	PostingSizes *PostingSizes                    // Stores the size of each posting
	Quantizer    *compressionhelpers.RotationalQuantizer
	Distancer    distancer.Provider
	id           string
	targetVector string
	rootPath     string

	// ctx and cancel are used to manage the lifecycle of the LocalRebuilder.
	ctx    context.Context
	cancel context.CancelFunc

	splitCh    chan splitOperation    // Channel for split operations
	mergeCh    chan mergeOperation    // Channel for merge operations
	reassignCh chan reassignOperation // Channel for reassign operations
	wg         sync.WaitGroup

	splitList *deduplicator // Prevents duplicate split operations
	mergeList *deduplicator // Prevents duplicate merge operations

	visitedPool *visited.Pool

	postingLocks *common.HashedLocks // Locks to prevent concurrent modifications to the same posting.

	dims                atomic.Int32
	vectorSize          int32 // Size of the compressed vectors in bytes
	trackDimensionsOnce sync.Once
	distancer           distancer.Provider
}

func (s *SPFresh) Start(ctx context.Context) {
	if s.UserConfig == nil {
		panic("UserConfig must be set before starting LocalRebuilder")
	}
	if s.Store == nil {
		panic("Store must be set before starting LocalRebuilder")
	}
	if s.VersionMap == nil {
		panic("VersionMap must be set before starting LocalRebuilder")
	}
	if s.IDs == nil {
		panic("IdGenerator must be set before starting LocalRebuilder")
	}
	if s.Quantizer == nil {
		panic("Quantizer must be set before starting LocalRebuilder")
	}

	s.ctx, s.cancel = context.WithCancel(ctx)

	if s.Logger == nil {
		s.Logger = logrus.New().WithField("component", "LocalRebuilder")
	} else {
		s.Logger = s.Logger.WithField("component", "LocalRebuilder")
	}

	s.postingLocks = common.NewHashedLocks32k()
	s.splitCh = make(chan splitOperation, s.UserConfig.SplitWorkers*100)          // TODO: fine-tune buffer size
	s.mergeCh = make(chan mergeOperation, 1024)                                   // TODO: fine-tune buffer size
	s.reassignCh = make(chan reassignOperation, s.UserConfig.ReassignWorkers*100) // TODO: fine-tune buffer size
	s.splitList = newDeduplicator()
	s.mergeList = newDeduplicator()
	s.visitedPool = visited.NewPool(1, 512, -1)
	s.distancer = distancer.NewCosineDistanceProvider()

	// start N workers to process split operations
	for i := 0; i < s.UserConfig.SplitWorkers; i++ {
		s.wg.Add(1)
		enterrors.GoWrapper(s.splitWorker, s.Logger)
	}

	// start M workers to process reassign operations
	for i := 0; i < s.UserConfig.ReassignWorkers; i++ {
		s.wg.Add(1)
		enterrors.GoWrapper(s.reassignWorker, s.Logger)
	}

	// start a single worker to process merge operations
	s.wg.Add(1)
	enterrors.GoWrapper(s.mergeWorker, s.Logger)
}

// Delete marks a vector as deleted in the version map.
func (s *SPFresh) Delete(ids ...uint64) error {
	for _, id := range ids {
		version := s.VersionMap.MarkDeleted(id)
		if version == 0 {
			return ErrVectorNotFound
		}
	}
	return nil
}

func (s *SPFresh) Type() common.IndexType {
	return common.IndexTypeSPFresh
}

func (s *SPFresh) UpdateUserConfig(updated schemaConfig.VectorIndexConfig, callback func()) error {
	return errors.New("UpdateUserConfig is not supported for the spfresh index")
}

func (s *SPFresh) Drop(ctx context.Context) error {
	_ = s.Shutdown(ctx)
	// Shard::drop will take care of handling store buckets
	return nil
}

func (s *SPFresh) Shutdown(ctx context.Context) error {
	if s.ctx == nil {
		return nil // Already closed or not started
	}

	if s.ctx.Err() != nil {
		return s.ctx.Err() // Context already cancelled
	}

	// Cancel the context to prevent new operations from being enqueued
	s.cancel()

	// Close the split channel to signal workers to stop
	close(s.splitCh)
	close(s.mergeCh)
	close(s.reassignCh)

	s.wg.Wait() // Wait for all workers to finish
	return nil
}

func (s *SPFresh) Flush() error {
	return nil
}

func (s *SPFresh) SwitchCommitLogs(ctx context.Context) error {
	return nil
}

func (s *SPFresh) ListFiles(ctx context.Context, basePath string) ([]string, error) {
	return nil, nil
}

func (s *SPFresh) PostStartup() {
	// This method can be used to perform any post-startup initialization
	// For now, it does nothing
}

func (s *SPFresh) Compressed() bool {
	return true
}

func (s *SPFresh) Multivector() bool {
	return false
}

func (s *SPFresh) ContainsDoc(id uint64) bool {
	v := s.VersionMap.Get(id)
	return !v.Deleted() && v.Version() > 0
}

func (s *SPFresh) Iterate(fn func(id uint64) bool) {
	s.Logger.Warn("Iterate is not implemented for SPFresh index")
}

func (s *SPFresh) getBucketName() string {
	if s.targetVector != "" {
		return fmt.Sprintf("%s_%s", helpers.VectorsBucketLSM, s.targetVector)
	}
	return helpers.VectorsBucketLSM
}

func float32SliceFromByteSlice(vector []byte, slice []float32) []float32 {
	for i := range slice {
		slice[i] = math.Float32frombits(binary.LittleEndian.Uint32(vector[i*4:]))
	}
	return slice
}

func (s *SPFresh) QueryVectorDistancer(queryVector []float32) common.QueryVectorDistancer {
	distFunc := func(id uint64) (float32, error) {
		var buf [8]byte
		binary.BigEndian.PutUint64(buf[:], id)
		vec, err := s.Store.store.Bucket(s.getBucketName()).Get(buf[:])
		if err != nil {
			return 0, err
		}

		dist, err := s.distancer.SingleDist(queryVector, float32SliceFromByteSlice(vec, make([]float32, len(vec)/4)))
		if err != nil {
			return 0, err
		}
		return dist, nil
	}

	return common.QueryVectorDistancer{DistanceFunc: distFunc}
}

func (s *SPFresh) CompressionStats() compressionhelpers.CompressionStats {
	return s.Quantizer.Stats()
}

// deduplicator is a simple structure to prevent duplicate values
type deduplicator struct {
	m *xsync.Map[uint64, struct{}]
}

func newDeduplicator() *deduplicator {
	return &deduplicator{
		m: xsync.NewMap[uint64, struct{}](),
	}
}

// tryAdd attempts to add an ID to the deduplicator.
// Returns true if the ID was added, false if it already exists.
func (d *deduplicator) tryAdd(id uint64) bool {
	_, exists := d.m.Compute(id, func(oldValue struct{}, loaded bool) (newValue struct{}, op xsync.ComputeOp) {
		if loaded {
			return oldValue, xsync.CancelOp
		}
		return struct{}{}, xsync.UpdateOp
	})
	return !exists
}

// done marks an ID as processed, removing it from the deduplicator.
func (d *deduplicator) done(id uint64) {
	d.m.Delete(id)
}

// contains checks if an ID is already in the deduplicator.
func (d *deduplicator) contains(id uint64) bool {
	_, exists := d.m.Load(id)
	return exists
}
