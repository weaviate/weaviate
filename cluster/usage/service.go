//                           _       _
// __      _____  __ ___   ___  __ _| |_ ___
// \ \ /\ / / _ \/ _` \ \ / / |/ _` | __/ _ \
//  \ V  V /  __/ (_| |\ V /| | (_| | ||  __/
//   \_/\_/ \___|\__,_| \_/ |_|\__,_|\__\___|
//
//  Copyright Â© 2016 - 2025 Weaviate B.V. All rights reserved.
//
//  CONTACT: hello@weaviate.io
//

package usage

import (
	"context"
	"strings"
	"time"

	"github.com/sirupsen/logrus"

	"github.com/weaviate/weaviate/adapters/repos/db"
	"github.com/weaviate/weaviate/adapters/repos/db/vector/common"
	"github.com/weaviate/weaviate/adapters/repos/db/vector/dynamic"
	"github.com/weaviate/weaviate/cluster/usage/types"
	backupent "github.com/weaviate/weaviate/entities/backup"
	"github.com/weaviate/weaviate/entities/models"
	entschema "github.com/weaviate/weaviate/entities/schema"
	schemaConfig "github.com/weaviate/weaviate/entities/schema/config"
	"github.com/weaviate/weaviate/entities/storagestate"
	"github.com/weaviate/weaviate/usecases/backup"
	"github.com/weaviate/weaviate/usecases/schema"
)

type Service interface {
	Usage(ctx context.Context) (*types.Report, error)
	SetJitterInterval(interval time.Duration)
}

type service struct {
	schemaManager  schema.SchemaGetter
	db             db.IndexGetter
	backups        backup.BackupBackendProvider
	logger         logrus.FieldLogger
	jitterInterval time.Duration
}

func NewService(schemaManager schema.SchemaGetter, db db.IndexGetter, backups backup.BackupBackendProvider, logger logrus.FieldLogger) Service {
	return &service{
		schemaManager:  schemaManager,
		db:             db,
		backups:        backups,
		logger:         logger,
		jitterInterval: 0, // Default to no jitter
	}
}

// SetJitterInterval sets the jitter interval for shard processing
func (s *service) SetJitterInterval(interval time.Duration) {
	s.jitterInterval = interval
	s.logger.WithFields(logrus.Fields{"jitter_interval": interval.String()}).Info("shard jitter interval updated")
}

// addJitter adds a small random delay if jitter interval is set
func (s *service) addJitter() {
	if s.jitterInterval <= 0 {
		return // No jitter if interval is 0 or negative
	}
	jitter := time.Duration(time.Now().UnixNano() % int64(s.jitterInterval))
	time.Sleep(jitter)
}

// Usage service collects usage metrics for the node and shall return error in case of any error
// to avoid reporting partial data
func (m *service) Usage(ctx context.Context) (*types.Report, error) {
	collections := m.schemaManager.GetSchemaSkipAuth().Objects.Classes
	usage := &types.Report{
		Node:        m.schemaManager.NodeName(),
		Collections: make([]*types.CollectionUsage, 0, len(collections)),
		Backups:     make([]*types.BackupUsage, 0),
	}

	// Collect usage for each collection
	for _, collection := range collections {
		shardingState := m.schemaManager.CopyShardingState(collection.Class)
		collectionUsage := &types.CollectionUsage{
			Name:              collection.Class,
			ReplicationFactor: int(collection.ReplicationConfig.Factor),
			UniqueShardCount:  int(len(shardingState.Physical)),
		}
		// Get shard usage
		index := m.db.GetIndexLike(entschema.ClassName(collection.Class))
		if index != nil {
			// First, collect cold tenants from sharding state
			for shardName, physical := range shardingState.Physical {
				// skip non-local shards
				if !shardingState.IsLocalShard(shardName) {
					continue
				}

				// Only process COLD tenants here
				if physical.ActivityStatus() == models.TenantActivityStatusCOLD {
					// Add jitter between cold tenant processing (except for the first one)
					if len(collectionUsage.Shards) > 0 {
						m.addJitter()
					}

					shardUsage, err := calculateUnloadedShardUsage(ctx, index, shardName, collection.VectorConfig)
					if err != nil {
						return nil, err
					}

					collectionUsage.Shards = append(collectionUsage.Shards, shardUsage)
				}
			}

			// Then, collect hot tenants from loaded shards
			index.ForEachShard(func(shardName string, shard db.ShardLike) error {
				// skip non-local shards
				if !shardingState.IsLocalShard(shardName) {
					return nil
				}

				// Add jitter between hot shard processing (except for the first one)
				if len(collectionUsage.Shards) > 0 {
					m.addJitter()
				}

				// Check shard status without forcing load
				if shard.GetStatusNoLoad() == storagestate.StatusLoading {
					shardUsage, err := calculateUnloadedShardUsage(ctx, index, shardName, collection.VectorConfig)
					if err != nil {
						return err
					}
					collectionUsage.Shards = append(collectionUsage.Shards, shardUsage)
					return nil
				}

				objectStorageSize, err := shard.ObjectStorageSize(ctx)
				if err != nil {
					return err
				}
				objectCount, err := shard.ObjectCountAsync(ctx)
				if err != nil {
					return err
				}

				vectorStorageSize, err := shard.VectorStorageSize(ctx)
				if err != nil {
					return err
				}

				shardUsage := &types.ShardUsage{
					Name:                shardName,
					Status:              strings.ToLower(models.TenantActivityStatusACTIVE),
					ObjectsCount:        objectCount,
					ObjectsStorageBytes: uint64(objectStorageSize),
					VectorStorageBytes:  uint64(vectorStorageSize),
				}

				// Get vector usage for each named vector
				if err = shard.ForEachVectorIndex(func(targetVector string, vectorIndex db.VectorIndex) error {
					category := db.DimensionCategoryStandard // Default category
					indexType := ""

					if vectorIndexConfig, ok := collection.VectorIndexConfig.(schemaConfig.VectorIndexConfig); ok {
						category, _ = db.GetDimensionCategory(vectorIndexConfig)
						indexType = vectorIndexConfig.IndexType()
					}

					dimensionality, err := shard.DimensionsUsage(ctx, targetVector)
					if err != nil {
						return err
					}

					// For dynamic indexes, get the actual underlying index type
					if dynamicIndex, ok := vectorIndex.(dynamic.Index); ok {
						indexType = dynamicIndex.UnderlyingIndex().String()
					}

					vectorUsage := &types.VectorUsage{
						Name:                   targetVector,
						Compression:            category.String(),
						VectorIndexType:        indexType,
						IsDynamic:              common.IsDynamic(common.IndexType(indexType)),
						VectorCompressionRatio: vectorIndex.CompressionStats().CompressionRatio(dimensionality.Dimensions),
					}

					// Only add dimensionalities if there's valid data
					if dimensionality.Count > 0 || dimensionality.Dimensions > 0 {
						vectorUsage.Dimensionalities = append(vectorUsage.Dimensionalities, &types.Dimensionality{
							Dimensions: dimensionality.Dimensions,
							Count:      dimensionality.Count,
						})
					}

					shardUsage.NamedVectors = append(shardUsage.NamedVectors, vectorUsage)
					return nil
				}); err != nil {
					return err
				}

				collectionUsage.Shards = append(collectionUsage.Shards, shardUsage)
				return nil
			})
		}

		usage.Collections = append(usage.Collections, collectionUsage)
	}

	// Get backup usage from all enabled backup backends
	for _, backend := range m.backups.EnabledBackupBackends() {
		backups, err := backend.AllBackups(ctx)
		if err != nil {
			m.logger.WithError(err).WithFields(logrus.Fields{"backend": backend}).Error("failed to get backups from backend")
			return nil, err
		}

		for _, backup := range backups {
			if backup.Status != backupent.Success {
				continue
			}
			usage.Backups = append(usage.Backups, &types.BackupUsage{
				ID:             backup.ID,
				CompletionTime: backup.CompletedAt.Format(time.RFC3339),
				SizeInGib:      float64(backup.PreCompressionSizeBytes) / (1024 * 1024 * 1024), // Convert bytes to GiB
				Type:           string(backup.Status),
				Collections:    backup.Classes(),
			})
		}
	}
	return usage, nil
}

func calculateUnloadedShardUsage(ctx context.Context, index db.IndexLike, tenantName string, vectorConfig map[string]models.VectorConfig) (*types.ShardUsage, error) {
	// Cold tenant: calculate from disk without loading
	objectUsage, err := index.CalculateUnloadedObjectsMetrics(ctx, tenantName)
	if err != nil {
		return nil, err
	}

	vectorStorageSize, err := index.CalculateUnloadedVectorsMetrics(ctx, tenantName)
	if err != nil {
		return nil, err
	}

	shardUsage := &types.ShardUsage{
		Name:                tenantName,
		ObjectsCount:        objectUsage.Count,
		Status:              strings.ToLower(models.TenantActivityStatusINACTIVE),
		ObjectsStorageBytes: uint64(objectUsage.StorageBytes),
		VectorStorageBytes:  uint64(vectorStorageSize),
	}

	// Get named vector data for cold shards from schema configuration
	for targetVector, vectorConfig := range vectorConfig {
		if vectorIndexConfig, ok := vectorConfig.VectorIndexConfig.(schemaConfig.VectorIndexConfig); ok {
			category, _ := db.GetDimensionCategory(vectorIndexConfig)
			indexType := vectorIndexConfig.IndexType()

			// For cold shards, we can't get actual dimensionality from disk without loading
			// So we'll use a placeholder or estimate based on the schema
			vectorUsage := &types.VectorUsage{
				Name:                   targetVector,
				Compression:            category.String(),
				VectorIndexType:        indexType,
				IsDynamic:              common.IsDynamic(common.IndexType(indexType)),
				VectorCompressionRatio: 1.0, // Default ratio for cold shards
			}

			shardUsage.NamedVectors = append(shardUsage.NamedVectors, vectorUsage)
		}
	}
	return shardUsage, err
}
